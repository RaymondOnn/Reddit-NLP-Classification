{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a988433",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Executive-Summary\" data-toc-modified-id=\"Executive-Summary-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Executive Summary</a></span></li></ul></li><li><span><a href=\"#Part-0:-Overview-&amp;-Problem-Statement\" data-toc-modified-id=\"Part-0:-Overview-&amp;-Problem-Statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Part 0: Overview &amp; Problem Statement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Background-/-Overview\" data-toc-modified-id=\"Background-/-Overview-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Background / Overview</a></span></li><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Problem Statement</a></span></li></ul></li><li><span><a href=\"#Part-1:-Webscraping-with-PushShift-API\" data-toc-modified-id=\"Part-1:-Webscraping-with-PushShift-API-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Part 1: Webscraping with PushShift API</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Function\" data-toc-modified-id=\"The-Function-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>The Function</a></span></li><li><span><a href=\"#The-Execution\" data-toc-modified-id=\"The-Execution-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The Execution</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653a290",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199b513",
   "metadata": {},
   "source": [
    "A cryptocurrency is a digital currency that is secured by cryptography. Bitcoin and Ethereum are the two largest cryptocurrencies by market capitalization as of this moment. While the top two coins share some similarities, they are also different in many ways. Investors and traders who wish to know more about these two top cryptocurrencies may find it difficult to grasp the many terminologies and jargons used in this field. \n",
    "\n",
    "A natural language processing classifier will be trained on posts from the two respective subreddits. It will learn to identify keywords that are more commonly associated with or are unique to each coin. This will be used to help businesses and enterprises develop an efficient and sophisticated query-answering and routing algorithm for their online chatbot to help handle the large number of enquiries. \n",
    "\n",
    "A total of 12 vectorizer-model combinations were evaluated. The vectorizers considered were  Count Vectorizer with binary=True,  Count Vectorizer with binary=False  and Tfidf Vectorizer. The models considered were Bernoulli & Multinomial Naive Bayes, Support Vector Machine, and Logistic Regression. Evaluation of the combinations was conducted using 2 metrics: accuracy and receiver operating characteristic area under curve (ROC AUC).\n",
    "\n",
    "For each combination. the text feature was first preprocessed using one of the text vectorizers before being passed into GridSearchCV to find the optimal collection of hyperparameters. Next, with the optimized model, cross validation was done with the train dataset and the model was subsequently tested using the test dataset.\n",
    "\n",
    "All in all, Tfidf Vectorizer-Logistic Regression with Ngram range( 1,2) is the combination of choice. It had the best test ROC AUC score at 0.9204 and while text accuracy is ~0.4% lower than the best test accuracy score at 83.97% (achieved with Ngram range(1,3)), the tradeoff in test accuracy was worth it for lower computational cost.\n",
    "The top 3 words that predicted a post to be from the Bitcoin subreddit were ‘bitcoin’, ‘lightning’, and 'year' while the top 3 words that predicted a post to be from the Ethereum subreddit were ‘ethereum’, ‘eth’, and ‘nft’. \n",
    "\n",
    "With the top words that the classifier has found for each subreddit, a minimum viable product can be designed. In its implementation, this chatbot will pick up on the keywords in a user-submitted message and try to identify whether the query pertains to Bitcoin or Ethereum. It will then give a suitable answer or route the message to the right customer service representative for further management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20171d-f825-4724-95fc-047586bf7ad9",
   "metadata": {},
   "source": [
    "# Part 0: Overview & Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4eacde",
   "metadata": {},
   "source": [
    "## Background / Overview\n",
    "\n",
    "We work for a cryptocurrency trading platform startup company. \\\n",
    "In the recent months, the customer service team has received an increasing number of enquries on the the cryptocurrencies available on our platform. On closer look, they found that a large proportion of these enquiries are related to what those cryptocurrencies are and their applications. \\\n",
    "Faced with increasing workload and resource constraints, the head of customer service has engaged our team to develop a real time chatbot for the company website to automate the process of responding to such simple enquiries. A real time chatbot will not only enable the customer service team to focus on complex enquiries or feedback, it can also help to educate users more timely and accurately on our products and hence enhance their user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eea9b5",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "This project aims to help businesses and enterprises in the cryptocurrency space (e.g. news provider, brokerage platforms, coin vaults, mining pools) develop an efficient and sophisticated query-answering and routing algorithm for their online chatbot to help handle the large number of enquiries received from site visitors on a daily basis and reduce the burden on customer service operatives. \n",
    "\n",
    "The goal is to empower the chatbot to be able to accurately determine the nature of the enquiry and return an appropriate answer; where it is unable to do so, it will categorise the class of the enquiry and route to the relevant operative. For this to happen, the chatbot needs to (for a start) know how to recognise keywords from two well-known cryptocurrencies (Bitcoin and Ethereum) with the help of a natural language processing classifier trained on posts from the two respective subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504dfcf8-76ab-4287-84ac-ea64777e8bc9",
   "metadata": {},
   "source": [
    "# Part 1: Webscraping with PushShift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70a596ad-b41c-4477-b30a-f9ef9f8fb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4685e5",
   "metadata": {},
   "source": [
    "## The Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "77f2cad4-cee4-4a36-aaa1-44b532e9624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(sub:str, after: int=None, before: int=None, num_posts:int = 25):\n",
    "    \"\"\"Scrapes data from Reddit based on desired subreddit, utc_time and number of posts\n",
    "       Returns dataframe with all scraped data loaded in\"\"\"\n",
    "    \n",
    "    # initialise variables\n",
    "    data = pd.DataFrame() \n",
    "    utc_before = before \n",
    "    utc_after = after\n",
    "    informed = False \n",
    "    sub = sub.lower()\n",
    "    row_count = 0\n",
    "    \n",
    "    \n",
    "    while row_count < num_posts:\n",
    "        # Doing a try / except loop just in case of rejection, we still retain what we have scraped in memory\n",
    "        try:\n",
    "            # Scraping reddit data. 100 posts per trigger should be relatively light on the server\n",
    "            with requests.Session() as s:\n",
    "                BASE_URL = \"https://api.pushshift.io/reddit/search/submission?subreddit=\" + sub\n",
    "                params = {'before': utc_before, 'after': utc_after, 'size': 100}\n",
    "                r = s.get(BASE_URL, params = params)\n",
    "\n",
    "            # User Feedback----------------------------------------------------------\n",
    "            if not informed:\n",
    "                print('Scraping data from {}'.format(BASE_URL)) \n",
    "                informed = True\n",
    "            print(\"UTC_Before: {}, Status: {}({})\".format(utc_before, r.reason, r.status_code))\n",
    "\n",
    "            # Loading data into dataframe---------------------------------------------\n",
    "            df = pd.DataFrame(r.json()['data'])\n",
    "            data = pd.concat([data, df], axis='rows').reset_index(drop=True)\n",
    "            \n",
    "        # if error, save current data and utc time    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return data, utc_before\n",
    "        else:\n",
    "            # Update variables for the next loop\n",
    "            row_count = data.shape[0]\n",
    "            utc_before = df['created_utc'].min() if utc_before != None else None\n",
    "            utc_after = df['created_utc'].max() if utc_after != None else None\n",
    "            print(\"Scraped {} rows. {} rows remaining\".format(row_count, num_posts - row_count))\n",
    "            \n",
    "            # check if condition is fulfilled else wait some secs before triggering next scrape\n",
    "            if num_posts - row_count != 0:\n",
    "                wait_time = random.randint(5, 20)\n",
    "                print('Waiting {} secs before next scrape'.format(wait_time))\n",
    "                print(\"-\"*100)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    print(\"Scraping Completed. Collected {} records.\".format(row_count))\n",
    "    print(\"_\"*100)\n",
    "    return data, utc_before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19a6ae",
   "metadata": {},
   "source": [
    "## The Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "77fe9cf6-a391-423f-a95a-a659e261854a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1626939127, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626907956, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626891406, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626874469, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 11 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626850198, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626823332, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626800951, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626780679, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626754656, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626722341, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1626703174\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1626939643, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626862969, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626789565, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626711159, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 19 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626621618, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626519850, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626443917, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626359543, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 16 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626279323, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626217156, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1626175345\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1626703174, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626666740, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626637198, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626609969, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626568615, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626542162, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626514540, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626470758, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 16 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626446699, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626421595, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1626387368\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1626175345, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626100128, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626011669, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625922043, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625846089, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625768120, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625712474, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625662214, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625600305, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625557033, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1625482616\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1626387368, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626365982, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 10 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626346730, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626314627, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626292439, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 19 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626273415, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626250869, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626224271, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626198598, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626181071, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1626154122\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1625482616, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 19 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625405341, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625315838, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625233092, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625154987, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625073565, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624998496, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624930898, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624880983, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624793204, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1624704597\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1626154122, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626117672, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626093065, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626058424, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626026547, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626003197, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625963983, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 19 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625930726, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625899269, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 16 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625862637, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1625839461\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1624704597, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624630192, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 10 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624555920, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624497292, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624436305, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624376642, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624318724, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624263637, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624178508, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624106100, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1624038499\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1625839461, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625817881, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625787682, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625766296, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625743469, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 10 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625706185, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625679151, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625658868, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625623705, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625597622, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1625575341\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1624038499, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623963599, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623915269, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 10 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623846906, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 11 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623774797, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623703103, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623658448, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623612332, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623541224, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623468780, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1623400734\n"
     ]
    }
   ],
   "source": [
    "# setup variables\n",
    "BTC_UTC = 1626939127\n",
    "ETH_UTC = 1626939643\n",
    "NUM_POSTS = 1000\n",
    "\n",
    "\n",
    "# Doing 1000 posts and then writing to file to prevent data loss\n",
    "# Also switching between subreddit to hopefully minimize getting ban\n",
    "df, btc_before1 = scrape_data(sub='Bitcoin', before=BTC_UTC, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before1))\n",
    "df.to_csv('bitcoin01.csv', index=False)\n",
    "df, eth_before1 = scrape_data(sub='ethereum', before=ETH_UTC, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum04.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before1))\n",
    "\n",
    "df, btc_before2 = scrape_data(sub='Bitcoin', before=btc_before1, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before2))\n",
    "df.to_csv('bitcoin05.csv', index=False)\n",
    "df, eth_before2 = scrape_data(sub='ethereum', before=eth_before1, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum05.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before2))\n",
    "\n",
    "df, btc_before3 = scrape_data(sub='Bitcoin', before=btc_before2, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before3))\n",
    "df.to_csv('bitcoin03.csv', index=False)\n",
    "df, eth_before3 = scrape_data(sub='ethereum', before=eth_before2, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum03.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before3))\n",
    "\n",
    "df, btc_before4 = scrape_data(sub='Bitcoin', before=btc_before3, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before4))\n",
    "df.to_csv('bitcoin04.csv', index=False)\n",
    "df, eth_before4 = scrape_data(sub='ethereum', before=eth_before3, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum04.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before4))\n",
    "\n",
    "df, btc_before5 = scrape_data(sub='Bitcoin', before=btc_before4, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before5))\n",
    "df.to_csv('bitcoin05.csv', index=False)\n",
    "df, eth_before5 = scrape_data(sub='ethereum', before=eth_before4, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum05.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4f64a-f13e-4082-8f96-88008fdeaa65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
