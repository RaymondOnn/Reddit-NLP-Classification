{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a988433",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Part-0:-Overview-&amp;-Problem-Statement\" data-toc-modified-id=\"Part-0:-Overview-&amp;-Problem-Statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Part 0: Overview &amp; Problem Statement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem Statement</a></span></li><li><span><a href=\"#Background-/-Overview\" data-toc-modified-id=\"Background-/-Overview-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Background / Overview</a></span></li></ul></li><li><span><a href=\"#Part-1:-Webscraping-with-PushShift-API\" data-toc-modified-id=\"Part-1:-Webscraping-with-PushShift-API-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Part 1: Webscraping with PushShift API</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Function\" data-toc-modified-id=\"The-Function-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>The Function</a></span></li><li><span><a href=\"#The-Execution\" data-toc-modified-id=\"The-Execution-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The Execution</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20171d-f825-4724-95fc-047586bf7ad9",
   "metadata": {},
   "source": [
    "# Part 0: Overview & Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91bdff-0a89-47b1-a114-4e8bee560132",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "1. Use natural language processing (NLP) to analyze the text data in each of the subreddits,\n",
    "2. Use machine learning (ML) classifiers to correctly classify the subreddit a submission is likely to originate from,\n",
    "3. Evaluate the ML classifiers against our baseline model using accuracy as the key metric and,\n",
    "4. Propose a suitable optimal ML classifier that could be used to develop a minimum viable product (MVP) for the chatbot and make other recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4eacde",
   "metadata": {},
   "source": [
    "## Background / Overview\n",
    "\n",
    "We work for a cryptocurrency trading platform startup company. \\\n",
    "In the recent months, the customer service team has received an increasing number of enquries on the the cryptocurrencies available on our platform. On closer look, they found that a large proportion of these enquiries are related to what those cryptocurrencies are and their applications. \\\n",
    "Faced with increasing workload and resource constraints, the head of customer service has engaged our team to develop a real time chatbot for the company website to automate the process of responding to such simple enquiries. A real time chatbot will not only enable the customer service team to focus on complex enquiries or feedback, it can also help to educate users more timely and accurately on our products and hence enhance their user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504dfcf8-76ab-4287-84ac-ea64777e8bc9",
   "metadata": {},
   "source": [
    "# Part 1: Webscraping with PushShift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70a596ad-b41c-4477-b30a-f9ef9f8fb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4685e5",
   "metadata": {},
   "source": [
    "## The Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "77f2cad4-cee4-4a36-aaa1-44b532e9624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(sub:str, after: int=None, before: int=None, num_posts:int = 25):\n",
    "    \"\"\"Scrapes data from Reddit based on desired subreddit, utc_time and number of posts\n",
    "       Returns dataframe with all scraped data loaded in\"\"\"\n",
    "    \n",
    "    # initialise variables\n",
    "    data = pd.DataFrame() \n",
    "    utc_before = before \n",
    "    utc_after = after\n",
    "    informed = False \n",
    "    sub = sub.lower()\n",
    "    row_count = 0\n",
    "    \n",
    "    \n",
    "    while row_count < num_posts:\n",
    "        # Doing a try / except loop just in case of rejection, we still retain what we have scraped in memory\n",
    "        try:\n",
    "            # Scraping reddit data. 100 posts per trigger should be relatively light on the server\n",
    "            with requests.Session() as s:\n",
    "                BASE_URL = \"https://api.pushshift.io/reddit/search/submission?subreddit=\" + sub\n",
    "                params = {'before': utc_before, 'after': utc_after, 'size': 100}\n",
    "                r = s.get(BASE_URL, params = params)\n",
    "\n",
    "            # User Feedback----------------------------------------------------------\n",
    "            if not informed:\n",
    "                print('Scraping data from {}'.format(BASE_URL)) \n",
    "                informed = True\n",
    "            print(\"UTC_Before: {}, Status: {}({})\".format(utc_before, r.reason, r.status_code))\n",
    "\n",
    "            # Loading data into dataframe---------------------------------------------\n",
    "            df = pd.DataFrame(r.json()['data'])\n",
    "            data = pd.concat([data, df], axis='rows').reset_index(drop=True)\n",
    "            \n",
    "        # if error, save current data and utc time    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return data, utc_before\n",
    "        else:\n",
    "            # Update variables for the next loop\n",
    "            row_count = data.shape[0]\n",
    "            utc_before = df['created_utc'].min() if utc_before != None else None\n",
    "            utc_after = df['created_utc'].max() if utc_after != None else None\n",
    "            print(\"Scraped {} rows. {} rows remaining\".format(row_count, num_posts - row_count))\n",
    "            \n",
    "            # check if condition is fulfilled else wait some secs before triggering next scrape\n",
    "            if num_posts - row_count != 0:\n",
    "                wait_time = random.randint(5, 20)\n",
    "                print('Waiting {} secs before next scrape'.format(wait_time))\n",
    "                print(\"-\"*100)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    print(\"Scraping Completed. Collected {} records.\".format(row_count))\n",
    "    print(\"_\"*100)\n",
    "    return data, utc_before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19a6ae",
   "metadata": {},
   "source": [
    "## The Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "77fe9cf6-a391-423f-a95a-a659e261854a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1626939127, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626907956, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626891406, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626874469, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 11 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626850198, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626823332, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626800951, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626780679, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626754656, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626722341, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1626703174\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1626939643, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626862969, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626789565, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626711159, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 19 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626621618, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626519850, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626443917, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626359543, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 16 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626279323, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626217156, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1626175345\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1626703174, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626666740, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626637198, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626609969, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626568615, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626542162, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626514540, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626470758, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 16 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626446699, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626421595, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1626387368\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1626175345, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626100128, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626011669, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625922043, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625846089, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625768120, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625712474, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625662214, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625600305, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625557033, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1625482616\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1626387368, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626365982, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 10 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626346730, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626314627, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626292439, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 19 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626273415, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626250869, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626224271, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626198598, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626181071, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1626154122\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1625482616, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 19 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625405341, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625315838, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625233092, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625154987, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625073565, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624998496, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624930898, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624880983, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624793204, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1624704597\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1626154122, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626117672, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626093065, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 9 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626058424, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626026547, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1626003197, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625963983, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 19 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625930726, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 18 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625899269, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 16 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625862637, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1625839461\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1624704597, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 7 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624630192, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 10 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624555920, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624497292, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624436305, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624376642, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624318724, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624263637, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 6 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624178508, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1624106100, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1624038499\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=bitcoin\n",
      "UTC_Before: 1625839461, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625817881, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625787682, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 17 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625766296, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 15 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625743469, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 10 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625706185, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625679151, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625658868, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 5 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625623705, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 20 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1625597622, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "BTC_UTC: 1625575341\n",
      "Scraping data from https://api.pushshift.io/reddit/search/submission?subreddit=ethereum\n",
      "UTC_Before: 1624038499, Status: OK(200)\n",
      "Scraped 100 rows. 900 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623963599, Status: OK(200)\n",
      "Scraped 200 rows. 800 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623915269, Status: OK(200)\n",
      "Scraped 300 rows. 700 rows remaining\n",
      "Waiting 10 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623846906, Status: OK(200)\n",
      "Scraped 400 rows. 600 rows remaining\n",
      "Waiting 11 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623774797, Status: OK(200)\n",
      "Scraped 500 rows. 500 rows remaining\n",
      "Waiting 8 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623703103, Status: OK(200)\n",
      "Scraped 600 rows. 400 rows remaining\n",
      "Waiting 14 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623658448, Status: OK(200)\n",
      "Scraped 700 rows. 300 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623612332, Status: OK(200)\n",
      "Scraped 800 rows. 200 rows remaining\n",
      "Waiting 13 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623541224, Status: OK(200)\n",
      "Scraped 900 rows. 100 rows remaining\n",
      "Waiting 12 secs before next scrape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UTC_Before: 1623468780, Status: OK(200)\n",
      "Scraped 1000 rows. 0 rows remaining\n",
      "Scraping Completed. Collected 1000 records.\n",
      "____________________________________________________________________________________________________\n",
      "ETH_UTC: 1623400734\n"
     ]
    }
   ],
   "source": [
    "# setup variables\n",
    "BTC_UTC = 1626939127\n",
    "ETH_UTC = 1626939643\n",
    "NUM_POSTS = 1000\n",
    "\n",
    "\n",
    "# Doing 1000 posts and then writing to file to prevent data loss\n",
    "# Also switching between subreddit to hopefully minimize getting ban\n",
    "df, btc_before1 = scrape_data(sub='Bitcoin', before=BTC_UTC, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before1))\n",
    "df.to_csv('bitcoin01.csv', index=False)\n",
    "df, eth_before1 = scrape_data(sub='ethereum', before=ETH_UTC, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum04.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before1))\n",
    "\n",
    "df, btc_before2 = scrape_data(sub='Bitcoin', before=btc_before1, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before2))\n",
    "df.to_csv('bitcoin05.csv', index=False)\n",
    "df, eth_before2 = scrape_data(sub='ethereum', before=eth_before1, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum05.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before2))\n",
    "\n",
    "df, btc_before3 = scrape_data(sub='Bitcoin', before=btc_before2, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before3))\n",
    "df.to_csv('bitcoin03.csv', index=False)\n",
    "df, eth_before3 = scrape_data(sub='ethereum', before=eth_before2, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum03.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before3))\n",
    "\n",
    "df, btc_before4 = scrape_data(sub='Bitcoin', before=btc_before3, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before4))\n",
    "df.to_csv('bitcoin04.csv', index=False)\n",
    "df, eth_before4 = scrape_data(sub='ethereum', before=eth_before3, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum04.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before4))\n",
    "\n",
    "df, btc_before5 = scrape_data(sub='Bitcoin', before=btc_before4, num_posts=NUM_POSTS)\n",
    "print('BTC_UTC: {}'.format(btc_before5))\n",
    "df.to_csv('bitcoin05.csv', index=False)\n",
    "df, eth_before5 = scrape_data(sub='ethereum', before=eth_before4, num_posts=NUM_POSTS)\n",
    "df.to_csv('ethereum05.csv', index=False)\n",
    "print('ETH_UTC: {}'.format(eth_before5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4f64a-f13e-4082-8f96-88008fdeaa65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
